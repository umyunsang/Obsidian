
---
## LLM 모델 이해
#### 자연어처리(NLP) 기술의 발전
![[Pasted image 20250120101836.png]]
- **n-gram 모델** : 연속된 단어의 그룹을 기반으로 확률을 계산하는 모델
- **Hidden Markov Model (HMM)** : 관찰할 수 없는(숨겨진) 상태와 관찰 가능한 데이터 사이
의 확률 관계를 모델링
- **Recurrent Neural Networks (RNN)** : 순차적 데이터(텍스트, 음성, 동영상 등)를 처리하기
위해 설계된 신경망 구조

![[Pasted image 20250120103621.png]]
- **Sequence-to-Sequence (Seq2Seq)** : Encoder(문맥이해, 압축)-Decoder(문장 생성) 아키텍 처 기반, 입력 시퀀스를 출력 시퀀스로 변환하는 모델 구조 
- **Attention** : 입력 시퀀스 내의 중요한 부분에 집중하여, 시퀀스 처리 작업의 성능을 향상 시키는 신경망 구조 
- **Transformer** : 어텐션 메커니즘을 활용하여 순차적 계산을 최소화하고 병렬 처리를 극대화 한 신경망 아키텍처

#### 자연어처리(NLP) Process

1. **Cleaning** : 불필요한 데이터 제거(예: HTML 태그, 특수 문자), 노이즈 감소 
	-  데이터의 품질을 보장하고 다음 단계의 처리를 용이하게 합니다. 
2. **Tokenization** : 텍스트를 작은 단위(토큰)로 나누는 과정 
3. **정규화** (Normalization) : 데이터의 일관성을 높이고, 처리 과정의 복잡성을 줄이고 텍스트 데이터의 변형을 최소화 
	-  어간 추출 (Stemming) : 단어에서 접사를 제거하여 기본 형태(어간)를 찾는 과정 
	-  표제어 추출 (Lemmatization): 단어를 사전에 등록된 형태(표제어)로 변환. 
	-  형태소 분석 (Morphological Analysis): 단어를 형태소로 분석하여 그 구조를 이해하는 과정. 
4. **품사 태깅** (Part-of-Speech Tagging): 각 토큰에 품사(명사, 동사 등)를 할당하는 과정 
	-  구문 분석이나 의미 분석에 중요한 정보를 제공 
5. **임베딩** (Embedding) : 토큰을 벡터 공간에 매핑하여 컴퓨터가 처리할 수 있는 수치적 형태로 변환. 
6. **학습** (Training) : 주어진 태스크(예: 분류, 번역)에 맞게 모델을 학습시키는 과정
	-  데이터를 기반으로 패턴을 학습하고 모델 파라미터를 최적화 
7. **생성 및 분류** (Generation and Classification)
