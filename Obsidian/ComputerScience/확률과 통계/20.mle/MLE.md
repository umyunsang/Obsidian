### 최대 우도 추정 (Maximum Likelihood Estimation, )

최대 우도 추정(MLE)은 관찰된 데이터를 가장 잘 설명하는 파라미터 $\theta$를 선택하는 알고리즘입니다. 우리가 파라미터를 추정하는 데 사용할 데이터는 $n$개의 독립적이고 동일하게 분포된(IID) 샘플 $X_1, X_2, \dots, X_n$입니다.

#### 우도 (Likelihood)

데이터가 동일하게 분포된다고 가정했으므로, 이들은 같은 확률 질량 함수(PMF) 또는 같은 확률 밀도 함수(PDF)를 가집니다. 우리는 $f(X|\theta)$로 이 공유된 PMF 또는 PDF를 나타낼 것입니다. 이 표기법은 두 가지 중요한 점을 포함합니다. 첫째, 우도가 파라미터 $\theta$에 따라 달라진다는 것을 나타내기 위해 조건부를 포함했습니다. 둘째, 이산 분포와 연속 분포 모두에 대해 동일한 기호 $f$를 사용합니다.

우도와 확률의 차이는 무엇일까요? 이산 분포의 경우, 우도는 데이터의 확률 질량 또는 결합 확률 질량과 동의어입니다. 연속 분포의 경우, 우도는 데이터의 확률 밀도를 나타냅니다.

각 데이터 포인트가 독립적이라고 가정했기 때문에, 모든 데이터의 우도는 각 데이터 포인트의 우도의 곱입니다. 수학적으로, 파라미터 $\theta$가 주어진 데이터의 우도는 다음과 같습니다:

$$
L(\theta) = \prod_{i=1}^n f(X_i|\theta)
$$

다른 파라미터 값에 대해 데이터의 우도는 달라집니다. 올바른 파라미터를 가지고 있다면, 데이터는 잘 맞을 것이고 그렇지 않은 경우는 그렇지 않을 것입니다. 따라서 우리는 우도를 파라미터 $\theta$의 함수로 작성합니다.

#### 최대화 (Maximization)

MLE의 목표는 이전 섹션의 우도 함수를 최대화하는 파라미터 $\theta$ 값을 선택하는 것입니다. 우리는 파라미터의 최적 선택을 나타내기 위해 $\hat{\theta}$를 사용할 것입니다. 형식적으로, MLE는 다음과 같이 가정합니다:

$$
\hat{\theta} = \underset{\theta}{\operatorname{argmax }} \text{ }L(\theta)
$$

argmax는 함수가 최대화되는 도메인의 값을 의미합니다. 이는 어떤 차원의 도메인에도 적용될 수 있습니다.

argmax의 멋진 속성은 로그가 단조 함수이기 때문에 함수의 로그의 argmax가 함수의 argmax와 동일하다는 것입니다. 로그는 수학을 더 간단하게 만들어줍니다. 따라서 MLE를 위해 먼저 로그 우도 함수(LL)를 작성합니다:

$$
LL(\theta) = \log L(\theta) = \log \prod_{i=1}^n f(X_i|\theta) = \sum_{i=1}^n \log f(X_i|\theta)
$$

최대 우도 추정기를 사용하려면, 먼저 주어진 파라미터의 로그 우도를 작성하고, 로그 우도 함수를 최대화하는 파라미터 값을 선택합니다. Argmax는 여러 가지 방법으로 계산할 수 있으며, 이 수업에서 다루는 모든 방법은 함수의 1차 도함수를 계산하는 것을 포함합니다.

### 베르누이 MLE 추정

첫 번째 예로, 우리는 베르누이 분포의 $p$ 파라미터를 추정하는 MLE를 사용할 것입니다. 우리는 $n$개의 데이터 포인트를 기반으로 추정할 것이며, 이를 IID 랜덤 변수 $X_1, X_2, \dots, X_n$으로 참조할 것입니다. 이 랜덤 변수들은 모두 동일한 베르누이 분포에서 샘플링된 것으로 가정됩니다: $X_i \sim \text{Ber}(p)$. 우리는 이 $p$ 값을 알아내고자 합니다.

MLE의 첫 번째 단계는 최적화할 수 있는 함수로서 베르누이의 우도를 작성하는 것입니다. 베르누이는 이산 분포이므로, 우도는 확률 질량 함수입니다.

베르누이 $X$의 확률 질량 함수는 다음과 같이 쓸 수 있습니다: $f(x) = p^x(1-p)^{1-x}$.

이제 MLE 추정을 해봅시다:

$$
L(\theta) = \prod_{i=1}^n p^{x_i}(1-p)^{1-x_i}
$$

$$
LL(\theta) = \sum_{i=1}^n \log p^{x_i}(1-p)^{1-x_i}$$
$$= \sum_{i=1}^n x_i (\log p) + (1 - x_i) \log(1-p)$$ 
$$= Y \log p + (n - Y) \log(1-p)
$$

여기서 $Y = \sum_{i=1}^n x_i$입니다.

이제 로그 우도 방정식을 얻었으므로, 로그 우도를 최대화하는 $p$ 값을 선택해야 합니다. 이를 위해 함수의 1차 도함수를 찾아 0으로 설정합니다:

$$
\frac{\partial LL(p)}{\partial p} = Y \frac{1}{p} + (n - Y) \frac{-1}{1-p} = 0
$$

따라서,

$$
\hat{p} = \frac{Y}{n} = \frac{\sum_{i=1}^n x_i}{n}$$

결국, MLE 추정값은 단순히 샘플 평균이 됩니다.

### 정규 MLE 추정

다음으로, 정규 분포의 최적 파라미터 값을 추정해 봅시다. 우리는 $n$개의 정규 분포에서 샘플링된 IID 랜덤 변수 $X_1, X_2, \dots, X_n$에 접근할 수 있습니다. 각 $X_i$는 $\mu = \theta_0, \sigma^2 = \theta_1$ 인 $N(\mu, \sigma^2)$에서 샘플링된 것으로 가정합니다. 이 경우 $\theta$는 평균( $\mu$ ) 및 분산( $\sigma^2$ )이라는 두 값을 가진 벡터입니다.

$$
L(\theta) = \prod_{i=1}^n f(X_i|\theta) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\theta_1}} e^{-\frac{(X_i - \theta_0)^2}{2\theta_1}}
$$

$$
LL(\theta) = \sum_{i=1}^n \log \frac{1}{\sqrt{2\pi\theta_1}} e^{-\frac{(X_i - \theta_0)^2}{2\theta_1}} = \sum_{i=1}^n \left[ - \log(\sqrt{2\pi\theta_1}) - \frac{1}{2\theta_1}(X_i - \theta_0)^2 \right]
$$

이제, 로그 우도 함수를 최대화하는 $\theta$ 값을 선택해야 합니다. 이를 위해 $LL$ 함수에 대해 $\theta_0$ 및 $\theta_1$에 대한 편미분을 계산하고 두 방정식을 모두 0으로 설정한 다음 $\theta$ 값을 구합니다. 그 결과는 다음과 같습니다:

$$
{\mu}_{MLE} = \frac{1}{n}\sum_{i=1}^n X_i, \quad {\sigma^2}_{MLE} = \frac{1}{n}\sum_{i=1}^n (x_i - {\mu}_{MLE})^2
$$

