
---
## **1. 벡터 연산 (Vector Operations)**

머신러닝에서는 데이터를 벡터(즉, 여러 개의 숫자를 하나의 덩어리로 다룸)로 표현하는 경우가 많아요.  
예를 들어,$x=(x1,x2,x3)\mathbf{x} = (x_1, x_2, x_3)$ 같은 벡터가 있다면, 우리는 이 벡터를 연산할 수 있습니다.

#### **벡터 덧셈과 뺄셈**

$$\mathbf{a} + \mathbf{b} = \begin{bmatrix} a_1 \\ a_2 \\ a_3 \end{bmatrix} + \begin{bmatrix} b_1 \\ b_2 \\ b_3 \end{bmatrix} = \begin{bmatrix} a_1 + b_1 \\ a_2 + b_2 \\ a_3 + b_3 \end{bmatrix}
$$
#### **스칼라 곱 (Scalar Multiplication)**

$$\mathbf{a} = c \begin{bmatrix} a_1 \\ a_2 \\ a_3 \end{bmatrix} = \begin{bmatrix} c a_1 \\ c a_2 \\ c a_3 \end{bmatrix}
$$
#### **L1 Norm & L2 Norm**

- **L1 Norm** (맨해튼 거리, 절댓값 합)

$$||\mathbf{x}||_1 = \sum_{i=1}^{n} |x_i|
$$
- **L2 Norm** (유클리드 거리)

$$||\mathbf{x}||_2 = \sqrt{\sum_{i=1}^{n} x_i^2}
$$
- L1 Norm은 희소성(sparsity)을 유도하여 **Lasso 회귀**에 사용되고,
- L2 Norm은 매끄러운 모델을 만들기 위해 **Ridge 회귀**에 사용됩니다.

---

## **2. 벡터 편미분과 Learning Rate**

#### **벡터 편미분 (Vector Partial Derivatives)**

머신러닝에서는 최적화(Optimization)를 위해 미분을 많이 사용합니다.

- **스칼라 함수의 벡터 미분 (Gradient)**

$$\nabla f(\mathbf{x}) = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{bmatrix}
$$
- **행렬의 미분**

$$\frac{\partial}{\partial \mathbf{x}} (\mathbf{x}^T \mathbf{A} \mathbf{x}) = (\mathbf{A} + \mathbf{A}^T) \mathbf{x}∂x∂​(xTAx)=(A+AT)x
$$
---

#### **경사 하강법(Gradient Descent)과 Learning Rate**

머신러닝에서 **손실 함수(loss function)**를 최소화하기 위해 **경사 하강법(Gradient Descent)**을 사용합니다.

업데이트 수식:

$$x:= \mathbf{x} - \eta \nabla f(\mathbf{x})
$$
- $\eta$ : **학습률(Learning Rate)**
- $\nabla f(\mathbf{x})$ : 손실 함수의 기울기

학습률이 너무 크면 최적점에서 튕겨 나가고, 너무 작으면 수렴 속도가 느려집니다.

---

## **3. 확률 변수 (Random Variable)**

확률 변수는 머신러닝에서 데이터를 확률적으로 모델링하는 데 사용됩니다.

#### **기댓값 (Expectation)**

$$E[X]= \sum_{i} x_i P(x_i)
$$
- 확률 변수 XX의 **평균적인 값**을 의미합니다.

#### **분산 (Variance)**

$$Var(X) = E[(X - E[X])^2] = E[X^2] - (E[X])^2
$$
- 데이터의 **흩어짐 정도(변동성)**를 측정합니다.

#### **표준편차 (Standard Deviation)**

$$\sigma(X) = \sqrt{Var(X)}
$$
- 분산의 제곱근이며, 원래 데이터와 같은 단위를 가집니다.

#### **확률 변수 연산 특징**

- $E[aX + b] = aE[X] + b$ 
- $Var(aX + b) = a^2Var(X)$

---

## **4. 확률 분포 (Probability Distributions)

확률 분포는 데이터가 어떤 패턴을 따르는지 설명하는 모델입니다.

#### **정규 분포 (Normal Distribution)**

$$P(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)
$$
- $\mu$ : 평균 (Mean)
- $\sigma^2$ : 분산 (Variance)
- 데이터가 **정규 분포**를 따른다면, 평균을 중심으로 대칭적인 형태를 가집니다.

---

#### **공분산 (Covariance)와 상관계수 (Correlation Coefficient)**

##### **공분산 (Covariance)**

$$Cov(X, Y) = E[(X - E[X])(Y - E[Y])]
$$
- 두 확률변수 $X,YX, Y$ 가 **어떻게 함께 변화하는지** 측정합니다.
- $Cov(X, Y) > 0$ 이면 **같이 증가**하고,
- $Cov(X, Y) < 0$ 이면 **반대 방향으로 변화**합니다.

##### **상관계수 (Correlation Coefficient)**

$$\rho(X, Y) = \frac{Cov(X, Y)}{\sigma_X \sigma_Y}
$$
- 공분산을 두 변수의 표준편차로 나눈 값으로,
- $\rho(X, Y) = 1$ 이면 완전히 같은 방향,
- $\rho(X, Y) = -1$ 이면 완전히 반대 방향입니다.

---

#### **다변량 정규분포 (Multivariate Normal Distribution)**

다변량 정규분포는 여러 개의 변수를 동시에 고려하는 확률분포입니다.

$$P(\mathbf{x}) = \frac{1}{(2\pi)^{\frac{n}{2}} |\Sigma|^{\frac{1}{2}}} \exp \left( -\frac{1}{2} (\mathbf{x} - \boldsymbol{\mu})^T \Sigma^{-1} (\mathbf{x} - \boldsymbol{\mu}) \right)
$$
- $\mathbf{x} = (x_1, x_2, \dots, x_n)$ : 다변량 확률 변수
- $\boldsymbol{\mu} = ( \mu_1, \mu_2, ..., \mu_n )$ : 평균 벡터
- $\Sigma$ : 공분산 행렬

이 분포는 **머신러닝에서 다차원 데이터의 분포를 모델링할 때** 매우 유용합니다.

---

