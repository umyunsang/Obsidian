
---
1. STFTë¥¼ ì œì™¸í•˜ê³  Anomal, Normal Dataë¥¼ í™•ì—°í•˜ê²Œ êµ¬ë³„í•  ìˆ˜ ìˆëŠ” ìƒˆë¡œìš´ Featureë°œêµ´í•˜ê¸°

---
## ğŸš€ ìƒˆë¡œìš´ Feature ë°œêµ´ ë°©ë²•

ASD (Anomaly Sound Detection) ì‹œìŠ¤í…œì—ì„œ **STFT(SHORT-TIME FOURIER TRANSFORM)** ì´ì™¸ì˜ íŠ¹ì§•(feature)ìœ¼ë¡œ **Anomalous Dataì™€ Normal Dataë¥¼ íš¨ê³¼ì ìœ¼ë¡œ êµ¬ë³„**í•˜ëŠ” ë°©ë²•ì„ ì†Œê°œí• ê²Œ.

---

## ğŸ”¥ 1. ë¹„ì£¼íŒŒìˆ˜ ë„ë©”ì¸ì—ì„œ ìƒˆë¡œìš´ Feature ë°œêµ´

STFTëŠ” ì‹œê°„-ì£¼íŒŒìˆ˜ ë¶„ì„ì— ì§‘ì¤‘í•˜ì§€ë§Œ, **ì‹œê°„ ë„ë©”ì¸(time domain)ê³¼ ë¹„ì„ í˜•ì (non-linear) íŠ¹ì§•**ì—ì„œë„ ì´ìƒ íƒì§€ì— ìœ ìš©í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ìˆì–´.

### ğŸ“Œ **1-1. í†µê³„ì  íŠ¹ì§• (Statistical Features)**

- **Mean, Variance, Skewness, Kurtosis**  
    â†’ ì´ìƒ ë°ì´í„°ëŠ” ë¶„í¬ì˜ í‰ê· ê³¼ ë¶„ì‚°ì´ ë‹¬ë¼ì§ˆ ê°€ëŠ¥ì„±ì´ ìˆìŒ.
    
- **Energy, Entropy**  
    â†’ ë¹„ì •ìƒì ì¸ ì†Œë¦¬ëŠ” ì—ë„ˆì§€ê°€ ê¸‰ê²©íˆ ì¦ê°€í•˜ê±°ë‚˜ ê°ì†Œí•  ìˆ˜ ìˆìŒ.
    
- **Zero Crossing Rate (ZCR)**  
    â†’ ì‹ í˜¸ê°€ 0ì„ ëª‡ ë²ˆì´ë‚˜ ì§€ë‚˜ê°€ëŠ”ì§€ í™•ì¸í•˜ë©´ ì •ìƒ/ë¹„ì •ìƒ íŒ¨í„´ì„ ì¡ì•„ë‚¼ ìˆ˜ ìˆìŒ.
    

---

### ğŸ“Œ **1-2. ì‹ í˜¸ì˜ ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„**

- **Autocorrelation**  
    â†’ ì •ìƒì ì¸ ì†Œë¦¬ëŠ” íŠ¹ì • ì£¼ê¸°ì„±ì„ ê°€ì§€ì§€ë§Œ, ì´ìƒ ë°ì´í„°ëŠ” ë¬´ì‘ìœ„ì„±ì„ ë¨ ê°€ëŠ¥ì„±ì´ ìˆìŒ.
    
- **Peak Analysis (Envelope)**  
    â†’ ì •ìƒ ì†Œë¦¬ëŠ” íŠ¹ì • íŒ¨í„´ì„ ê°€ì§€ì§€ë§Œ, ì´ìƒ ë°ì´í„°ëŠ” ê¸‰ê²©í•œ í”¼í¬(peak)ë‚˜ ë³€í™”ê°€ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìŒ.
    
- **Short-Time Energy (STE)**  
    â†’ íŠ¹ì • êµ¬ê°„ì—ì„œ ì—ë„ˆì§€ë¥¼ ê³„ì‚°í•˜ë©´ ì´ìƒ ë°ì´í„°ê°€ ê¸‰ê²©íˆ ë³€í•  ê°€ëŠ¥ì„±ì´ ë†’ìŒ.
    

---

### ğŸ“Œ **1-3. ë¹„ì„ í˜• íŠ¹ì§• í™œìš©**

- **Fractal Dimension**  
    â†’ ë¹„ì •ìƒ ì†Œë¦¬ëŠ” ë³µì¡ë„ê°€ ë†’ê±°ë‚˜ ë‚®ê²Œ ë³€í•  ìˆ˜ ìˆìŒ.
    
- **Lyapunov Exponent (ì¹´ì˜¤ìŠ¤ ì´ë¡  í™œìš©)**  
    â†’ ì‹œìŠ¤í…œì˜ ë¯¼ê°ë„ë¥¼ ì¸¡ì •í•˜ë©´ ì •ìƒ/ë¹„ì •ìƒ êµ¬ë¶„ì´ ê°€ëŠ¥í•  ìˆ˜ ìˆìŒ.
    
- **Hurst Exponent**  
    â†’ ì •ìƒì ì¸ ì‹ í˜¸ëŠ” íŠ¹ì • ìê¸° ìœ ì‚¬ì„±ì„ ê°€ì§€ì§€ë§Œ, ì´ìƒ ë°ì´í„°ëŠ” ê·¸ë ‡ì§€ ì•Šì„ ê°€ëŠ¥ì„±ì´ ìˆìŒ.
    

---

## ğŸ”¥ 2. ë¨¸ì‹ ëŸ¬ë‹ì„ í™œìš©í•œ Feature Selection

1. **Feature Engineering í›„ ë‹¤ì–‘í•œ Feature ì¶”ì¶œ**
    
    - ìœ„ì—ì„œ ì†Œê°œí•œ íŠ¹ì§•ì„ ì „ë¶€ ê³„ì‚°í•œ í›„ ë²¡í„°í™”
        
2. **ì°¨ì› ì¶•ì†Œ ê¸°ë²• ì ìš©**
    
    - PCA(ì£¼ì„±ë¶„ ë¶„ì„) ë˜ëŠ” LDA(ì„ í˜•íŒë³„ë¶„ì„)ë¥¼ í†µí•´ ì£¼ìš” Feature ì„ ì •
        
3. **Feature Importance í‰ê°€**
    
    - Random Forest, XGBoost ë“±ì„ í™œìš©í•˜ì—¬ ì¤‘ìš”í•œ Featureë¥¼ ì„ ë³„
        

---

## ğŸ”¥ 3. ë”¥ëŸ¬ë‹ì„ í™œìš©í•œ Feature í•™ìŠµ

ë§Œì•½ ìˆ˜ì‘ì—… Feature Selectionì´ ì–´ë µë‹¤ë©´ **ë”¥ëŸ¬ë‹ì„ í™œìš©**í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥í•¨.

1. **CNNìœ¼ë¡œ Mel-Spectrogram í™œìš© (STFT ëŒ€ì²´)**
    
    - STFT ëŒ€ì‹  **Mel-Spectrogram**ì„ í™œìš©í•˜ì—¬ CNN ê¸°ë°˜ ëª¨ë¸ì„ í›ˆë ¨
        
2. **Autoencoder ê¸°ë°˜ Feature í•™ìŠµ**
    
    - ì •ìƒ ë°ì´í„°ë§Œ í•™ìŠµí•œ í›„, ì´ìƒ ë°ì´í„°ì—ì„œ Reconstruction Errorê°€ ì»¤ì§€ëŠ”ì§€ í™•ì¸
        
3. **Contrastive Learning í™œìš©**
    
    - ì •ìƒ/ì´ìƒ ë°ì´í„°ë¥¼ Contrastive Lossë¥¼ í™œìš©í•˜ì—¬ Feature Representation í•™ìŠµ
        

---

## âœ… ê²°ë¡ 

STFT ì—†ì´ ì´ìƒ ë°ì´í„°ë¥¼ íƒì§€í•˜ë ¤ë©´ **í†µê³„ì  íŠ¹ì§•, ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„, ë¹„ì„ í˜• íŠ¹ì§•**ì„ í™œìš©í•˜ì—¬ ìƒˆë¡œìš´ Featureë¥¼ ë°œêµ´í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•´!  
ë˜í•œ ë¨¸ì‹ ëŸ¬ë‹ì„ í™œìš©í•œ Feature Selectionê³¼ ë”¥ëŸ¬ë‹ ê¸°ë°˜ Feature í•™ìŠµë„ íš¨ê³¼ì ì¸ ë°©ë²•ì´ ë  ìˆ˜ ìˆìŒ.

ì´ ë°©ë²• ì¤‘ì—ì„œ í”„ë¡œì íŠ¸ì— ì ìš©í•˜ê¸° ì ì ˆí•œ ë°©ë²•ì„ ì„ íƒí•´ì„œ ì‹¤í—˜í•´ ë³´ë©´ ì¢‹ì„ ê²ƒ ê°™ì•„! ğŸš€

---

ì•„ë˜ëŠ” ìƒˆë¡œìš´ Feature ë°œêµ´ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ì½”ë“œ ìƒ˜í”Œì„ ì¶”ì²œí•´ì¤„ê²Œ. ì£¼ë¡œ **Python**ì„ í™œìš©í•˜ë©°, ì—¬ëŸ¬ í†µê³„ì , ì‹œê³„ì—´ì , ë¹„ì„ í˜•ì  íŠ¹ì§•ì„ ì¶”ì¶œí•  ìˆ˜ ìˆëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì†Œê°œí•  ê±°ì•¼.

---

## 1. **í†µê³„ì  íŠ¹ì§• ì¶”ì¶œ ë¼ì´ë¸ŒëŸ¬ë¦¬**

### ğŸ“Œ **`scipy`, `numpy`**

`scipy`ì™€ `numpy`ë¥¼ í™œìš©í•˜ë©´ ê¸°ë³¸ì ì¸ í†µê³„ì  íŠ¹ì§•ì„ ì‰½ê²Œ ê³„ì‚°í•  ìˆ˜ ìˆì–´.

```python
import numpy as np
from scipy.stats import kurtosis, skew

def statistical_features(signal):
    mean = np.mean(signal)
    variance = np.var(signal)
    skewness = skew(signal)
    kurt = kurtosis(signal)
    return mean, variance, skewness, kurt
```

### ğŸ“Œ **`librosa`**

ìŒì•…/ì†Œë¦¬ ì‹ í˜¸ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ **Zero Crossing Rate, Energy, Entropy ë“±**ì„ ì‰½ê²Œ ì¶”ì¶œí•  ìˆ˜ ìˆì–´.

```python
import librosa
import numpy as np

def signal_features(file_path):
    signal, sr = librosa.load(file_path)

    # Zero Crossing Rate
    zcr = librosa.feature.zero_crossing_rate(signal).mean()

    # Spectral Centroid
    centroid = librosa.feature.spectral_centroid(y=signal, sr=sr).mean()

    # Spectral Roll-Off
    rolloff = librosa.feature.spectral_rolloff(y=signal, sr=sr).mean()

    # Spectral Flatness
    flatness = librosa.feature.spectral_flatness(y=signal).mean()

    return zcr, centroid, rolloff, flatness
```

---

## 2. **ì‹œê³„ì—´ íŒ¨í„´ ë¶„ì„**

### ğŸ“Œ **`statsmodels`**

ì‹œê³„ì—´ ë¶„ì„ì— ìœ ìš©í•œ `autocorrelation`ì„ ê³„ì‚°í•  ìˆ˜ ìˆì–´.

```python
import statsmodels.api as sm

def autocorrelation(signal, lags=40):
    acf = sm.tsa.acf(signal, nlags=lags)
    return acf
```

### ğŸ“Œ **`pywt` (Wavelet Transform)**

ì‹œê³„ì—´ ì‹ í˜¸ì˜ íŒ¨í„´ì„ ë¶„ì„í•˜ëŠ” ë° ìœ ìš©í•œ **Wavelet Transform**ì„ í™œìš©í•  ìˆ˜ ìˆì–´.

```python
import pywt
import numpy as np

def wavelet_transform(signal):
    coeffs = pywt.wavedec(signal, 'db1')  # db1 is Daubechies wavelet
    return coeffs
```

---

## 3. **ë¹„ì„ í˜•ì  íŠ¹ì§• í™œìš©**

### ğŸ“Œ **`nolds` (Fractal Dimension)**

í”„ë™íƒˆ ì°¨ì›ì„ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ `nolds`.

```python
import nolds

def fractal_dimension(signal):
    return nolds.dfa(signal)
```

### ğŸ“Œ **`chaospy` (Lyapunov Exponent)**

ì¹´ì˜¤ìŠ¤ ì´ë¡ ì„ ì ìš©í•  ìˆ˜ ìˆëŠ” **Lyapunov Exponent**ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆì–´.

```python
import chaospy as cp

def lyapunov_exponent(signal):
    # Lyapunov exponent ê³„ì‚° ë°©ë²•ì€ ì‹œìŠ¤í…œì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ.
    # ì˜ˆì‹œë¡œ ê°„ë‹¨íˆ êµ¬í˜„í•œ ë°©ì‹.
    return cp.stat.pearson(signal)  # Example, replace with actual Lyapunov calculation
```

---

## 4. **ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ Feature Selection**

### ğŸ“Œ **`sklearn`**

**Random Forest**, **XGBoost** ë“±ì„ í™œìš©í•´ ì¤‘ìš” Featureë¥¼ ì„ íƒí•˜ëŠ” ë°©ë²•ì„ ì†Œê°œí• ê²Œ.

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel

def feature_importance(X_train, y_train):
    clf = RandomForestClassifier()
    clf.fit(X_train, y_train)

    model = SelectFromModel(clf, threshold="mean", max_features=5)
    X_selected = model.transform(X_train)
    
    return X_selected, clf.feature_importances_
```

### ğŸ“Œ **`xgboost`**

`XGBoost`ë¥¼ í™œìš©í•œ feature importance ê³„ì‚° ì˜ˆì‹œ.

```python
import xgboost as xgb
from xgboost import plot_importance

def xgboost_feature_importance(X_train, y_train):
    model = xgb.XGBClassifier()
    model.fit(X_train, y_train)
    
    plot_importance(model, importance_type='weight')  # ì¤‘ìš” feature ì‹œê°í™”
    return model.feature_importances_
```

---

## 5. **ë”¥ëŸ¬ë‹ì„ í™œìš©í•œ Feature í•™ìŠµ**

### ğŸ“Œ **`tensorflow`, `keras`**

Autoencoder ê¸°ë°˜ìœ¼ë¡œ íŠ¹ì§•ì„ í•™ìŠµí•˜ê³  Reconstruction Errorë¥¼ í™œìš©í•˜ì—¬ ì´ìƒ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ëŠ” ë°©ë²•.

```python
import tensorflow as tf
from tensorflow.keras import layers

def autoencoder_model(input_dim):
    input_layer = layers.Input(shape=(input_dim,))
    encoded = layers.Dense(128, activation='relu')(input_layer)
    encoded = layers.Dense(64, activation='relu')(encoded)
    decoded = layers.Dense(128, activation='relu')(encoded)
    decoded = layers.Dense(input_dim, activation='sigmoid')(decoded)

    autoencoder = tf.keras.models.Model(input_layer, decoded)
    autoencoder.compile(optimizer='adam', loss='mean_squared_error')
    
    return autoencoder

# í•™ìŠµ í›„ Reconstruction Error ê³„ì‚°
def reconstruction_error(model, X_test):
    reconstructed = model.predict(X_test)
    error = np.mean(np.square(X_test - reconstructed), axis=1)
    return error
```

---

## 6. **Feature Selectionê³¼ ëª¨ë¸ í•™ìŠµì„ ê²°í•©í•œ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì˜ˆì‹œ**

```python
import librosa
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# ì‹ í˜¸ì—ì„œ íŠ¹ì§• ì¶”ì¶œ
def extract_features(file_paths):
    features = []
    labels = []
    for file_path in file_paths:
        signal, sr = librosa.load(file_path)
        zcr, centroid, rolloff, flatness = signal_features(file_path)
        features.append([zcr, centroid, rolloff, flatness])
        labels.append(0 if "normal" in file_path else 1)  # 0: Normal, 1: Anomalous
    return np.array(features), np.array(labels)

# ë°ì´í„° ë¡œë”© ë° íŠ¹ì§• ì¶”ì¶œ
file_paths = ["normal_1.wav", "normal_2.wav", "anomaly_1.wav"]
X, y = extract_features(file_paths)

# í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Random Forest ëª¨ë¸ í•™ìŠµ
model = RandomForestClassifier()
model.fit(X_train, y_train)

# ì˜ˆì¸¡ ë° í‰ê°€
y_pred = model.predict(X_test)
print("Accuracy:", accuracy_score(y_test, y_pred))
```

---

ì´ ì½”ë“œë“¤ì€ **íŠ¹ì§• ì¶”ì¶œ**ê³¼ **ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸**ì„ ê²°í•©í•˜ì—¬ ASD ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤„ ê±°ì•¼. ê° ë°©ë²•ì„ ì¡°í•©í•˜ê±°ë‚˜ í•„ìš”ì— ë”°ë¼ ìˆ˜ì •í•´ ê°€ë©° ì ìš©í•´ ë³´ê¸°ë¥¼ ê¶Œì¥í•´! ğŸ˜Š