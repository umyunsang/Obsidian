
---
## 활성화 함수(Activation Function) 정리

딥러닝에서 **활성화 함수(activation function)**는 뉴런의 출력값을 비선형적으로 변환하여, 신경망이 복잡한 함수(비선형성)를 학습할 수 있도록 해줍니다. 대표적인 활성화 함수들의 수식, 특성, 장단점을 정리합니다.

---

### 1. Sigmoid

- **수식:**  
  $$
  \sigma(x) = \frac{1}{1 + e^{-x}}
  $$
- **출력 범위:** (0, 1)
- **특징:**  
  - 출력이 확률처럼 해석 가능 (이진 분류 출력층에 자주 사용)
  - 입력이 커질수록 1, 작아질수록 0에 수렴
- **단점:**  
  - **Vanishing Gradient(기울기 소실)** 문제: 입력이 크거나 작으면 미분값이 0에 가까워져 학습이 잘 안 됨
  - 출력이 0 중심이 아님 → 학습 속도 저하

---

### 2. Tanh (Hyperbolic Tangent)

- **수식:**  
  $$
  \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
  $$
- **출력 범위:** (-1, 1)
- **특징:**  
  - Sigmoid와 유사하지만 0 중심
  - 은닉층에서 Sigmoid보다 더 자주 사용
- **단점:**  
  - 여전히 Vanishing Gradient 문제 존재

---

### 3. ReLU (Rectified Linear Unit)

- **수식:**  
  $$
  \mathrm{ReLU}(x) = \max(0, x)
  $$
- **출력 범위:** $[0, \infty)$
- **특징:**  
  - 계산이 매우 간단, 빠름
  - Vanishing Gradient 문제 완화
  - 딥러닝에서 가장 널리 사용
- **단점:**  
  - 입력이 0 이하일 때 gradient가 0 → **Dead Neuron** 문제

---

### 4. Leaky ReLU

- **수식:**  
  $$
  \mathrm{LeakyReLU}(x) = 
  \begin{cases}
    x & \text{if } x \geq 0 \\
    \alpha x & \text{if } x < 0
  \end{cases}
  $$
  (보통 $\alpha=0.01$)
- **출력 범위:** $(-\infty, \infty)$
- **특징:**  
  - ReLU의 Dead Neuron 문제를 완화
  - 음수 입력에 대해 작은 기울기($\alpha$)를 부여

---

### 5. ELU (Exponential Linear Unit)

- **수식:**  
  $$
  \mathrm{ELU}(x) = 
  \begin{cases}
    x & \text{if } x \geq 0 \\
    \alpha (e^x - 1) & \text{if } x < 0
  \end{cases}
  $$
  (보통 $\alpha=1$)
- **특징:**  
  - Leaky ReLU보다 음수 영역에서 더 부드러운 곡선
  - 출력이 0 중심에 가까움
  - 학습 속도 및 성능 개선 가능

---

### 6. Swish

- **수식:**  
  $$
  \mathrm{Swish}(x) = x \cdot \sigma(\beta x)
  $$
  (보통 $\beta=1$)
- **특징:**  
  - Google에서 제안
  - ReLU와 Sigmoid의 장점 결합
  - 비선형성이 더 부드러움, 성능이 더 좋을 수 있음

---

### 7. GELU (Gaussian Error Linear Unit)

- **수식:**  
  $$
  \mathrm{GELU}(x) = x \cdot \Phi(x)
  $$
  여기서 $\Phi(x)$는 표준 정규분포의 누적분포함수(CDF)
- **근사식:**  
  $$
  \mathrm{GELU}(x) \approx 0.5x \left[1 + \tanh\left(\sqrt{\frac{2}{\pi}}(x + 0.044715x^3)\right)\right]
  $$
- **특징:**  
  - Transformer 등 최신 모델에서 널리 사용
  - Swish와 유사하지만, 확률적 해석 가능
  - 입력값에 따라 일부만 통과시키는 효과

---

### **요약 표**

| 함수명      | 수식/특징 요약                | 출력 범위   | 장점/단점 요약                |
|-------------|-------------------------------|-------------|-------------------------------|
| Sigmoid     | $1/(1+e^{-x})$                | (0, 1)      | 확률 해석, 기울기 소실        |
| Tanh        | $(e^x-e^{-x})/(e^x+e^{-x})$   | (-1, 1)     | 0 중심, 기울기 소실           |
| ReLU        | $\max(0, x)$                  | $[0, \infty)$| 빠름, Dead Neuron             |
| Leaky ReLU  | $x$ or $\alpha x$             | $(-\infty, \infty)$ | Dead Neuron 완화      |
| ELU         | $x$ or $\alpha(e^x-1)$        | $(-\infty, \infty)$ | 0 중심, 부드러움      |
| Swish       | $x \cdot \sigma(x)$           | $(-\infty, \infty)$ | 부드러움, 성능↑        |
| GELU        | $x \cdot \Phi(x)$             | $(-\infty, \infty)$ | 최신 모델, 확률적 해석 |

---

